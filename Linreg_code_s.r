# Перед тем как загрузить датасет, удобно будет создать для работы с ним отдельную директорию на компьютере.
# Включаем R, проверяем в какой директории мы работаем сейчас:
getwd() 
# Меняем директорию на вновь созданную (если необходимо):
setwd("D:/Docs/PhD/Statistics MS/Linear regression") # В кавычках заполняем адрес новой директории
# Хорошей практикой будет удаление всех переменных, которые могли остаться в R после предыдущей сессии 
rm(list=ls())

# Устанавливаем необходимые для работы пакеты
install.packages("tidyverse")
install.packages("psych")
install.packages("corrgram")
install.packages("caret")

#Следующим шагом будет загрузка датасета. Те из Вас кто будет использовать датасеты, которые дал 
# Михаил Сергеевич, делают следующую команду (в кавычках название датасета):
data("iris") 
# , - и в дальнейшем обращаются к датасету по названию (в данном случае, iris)
# Те кто посмотрел датасет и обнаружил там временной ряд (данные индексированы временем), матрицу корреляций, ссылку на все датасеты в R -
# пожалуйста, напишите мне и я отправлю Вам новый подходящий датасет. Если Вы не уверены, подходит ли Ваш датасет -
# тоже пишите мне.
# Датасет, который я Вам пришлю, будет в форме ссылки на текст. Вы копируете целиком все данные и сохряняете их
# в формате .txt в Блокноте. Затем Вы копируете все необходимые данные в шапке и оставляете сохраняете их так, чтобы к ним
# был удобный доступ. Затем удаляете шапку и оставляете только укороченные названия параметров, разделенные пробелом, в той
# последовательности, в которой выстроены колонки с данными. За примером обращайтесь к моим исходным файлам.

# Напоминаю, что в рассматриваемом датасете мы пытаемся объяснить смертность в населенных пунктах (колонка B, зависимая переменная)
# как линейную функцию от регрессоров в колонках А1 - А15. В каждой колонке по 60 значений.

#17 columns
#60 rows
#Index
#A1 average annual precipitation in inches
#A2 average January temperature in degrees Fahrenheit
#A3 average July temperature in degrees Fahrenheit
#A4 percent of 1960 SMSA population 65 years old or older
#A5 household size, 1960
#A6 schooling for persons over 22
#A7 household with full kitchens
#A8 population per square mile in urbanized areas
#A9 percent nonwhite population
#A10 percent office workers
#A11 poor families (annual income under $3000)
#A12 relative pollution potential of hydrocarbons
#A13 relative pollution potential of oxides of Nitrogen
#A14 relative pollution of Sulfur Dioxide
#A15 percent relative humidity, annual average at 1pm.
#B death rate

# Загружаем датасет в R
dataset <- read.table("Death_rates_1.txt", header = TRUE)
dataset$Index <- NULL #Удаляем колонку с индексами

# Проверяем зависимую переменную на нормальность распределения. Это не обязательное условие применения линейной регрессии,
# однако нормальность распределения данных в целом делает работу гораздо более удобной
h1 <- hist(dataset$B, freq = FALSE) 
curve(dnorm(x, mean=mean(dataset$B), sd=sd(dataset$B)), add = TRUE) #здесь мы рисуем гистограмму и добавляем
# для наглядности кривую нормального распределения с параметрами средней и дисперсии, взятыми от зависимой переменной
# (таким образом очерчивая контуры гистограммы нормального распределения с данными параметрами).
# На гисторграмме видно что данные не совсем нормально распределены

# Пробуем логарифмическую трансформацию (для удобства сразу делаем трансформацию всего датасета), смотрим гистограмму
dataset_log <- log(dataset)
h2 <- hist(log(dataset$B), freq = FALSE)
curve(dnorm(x, mean=mean(log(dataset$B)), sd=sd(log(dataset$B))), add = TRUE)
# Распределение стало себя лучше вести, хотя оно все же не стало нормальным. Мы продолжим работать с оригинальным 
# датасетом (спойлер: лучше не заморачиваться и сразу работать с логарифмически трансформированным ;) )



# Этот шаг поможет нам ближе познакомится с характеристиками данных в датасете 
library(psych)
psych::describe(dataset)
# Далее мы этот шаг не используем, но он может в будущем пригодится



# В этом шаге мы построим удобную визуализацию, которая будет давать нам информацию сразу о пяти важных
# характеристиках каждого столбца данных: медиана, 1я и 3я квартиль, а также данные, попадающие в промежуток, 
# равный 1,5*IQR от каждой квартили (IQR - это межквартильный интервал). Эта же визуализация поможет легко
# обнаружить экстремальные значения в данных
library(reshape2)
library(ggplot2)
meltData <- melt(dataset) # строим индексированный 1D ряд из датасета, нужно для ggplot
p <- ggplot(meltData, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free") + coord_flip() 
# Почти во всех колонках данных присуствуют экстремальные значения. По-хорошему, с ними надо проводить отдельную 
# работу для того чтобы модель работала корректно. На семинаре мы этого не делали, поэтому здесь этого пока
# тоже нет. 
# Обратите внимания на экстремальные значения A12 и A13 - у них огромная магнитуда, которая на несколько порядков
# превышает межквартильный интервал. Эти колонки отражают загрязнение воздуха в городах углеводородом и оксидами азота соответственно - параметры, 
# которые могут варьироваться в зависимости от социально-экономического устройства населенного пунтка. Моногорода, крупные метрополии и индустриальные
# центры могут демонстрировать существенное превышение уровня загрязнений по сравнению с неиндустриализованными 
# населенными пунктами. 


# Для того чтобы выбрать какие регрессоры нам стоит включить в нашу модель, мы строим матрицу корреляций: 
require(corrgram)
c <- corrgram(dataset) # это стандартный вид матрицы корреляций 
c2 <- corrgram(dataset, order=TRUE, lower.panel = panel.pts, upper.panel = panel.pie)
# это более интересный вид матрицы корреляций, который дает нам немного дополнительной информации
# В правом треугольнике мы видим магнитуду корреляции в виде секторов кругов (что нагляднее плотности закрашивания в моем представлении),
# отрицательные значения обозначает красный цвет, положительные - синий. Левый нижний треугольник - это рассеянные графики
# распределения двух величин. Безусловно, для наглядной иллюстрации совместного распределения стоит использовать отдельно
# взятый график, как например:
plot(dataset$A12, dataset$A9)
# , - однако, даже в такой миниатюрной версии мы можем распознать устойчивые тренды зависимости, а также наличие отклонений - обратите внимание на 
# графики А12 и А13 - их распределения страдают от экстремальных значений.
# Еще одной удобной фичей является упорядочивание столбцов в матрице корреляций(order = TRUE). Упорядочивание происходит в соответствие с направлениями векторов "загрузки", отражающих
# долю участия столбцов данных в формировании принципиальных компонентов. Это позволяет выстроить матрицу корреляций таким образом, что большие значения корреляции
# оказываются, в большинстве случаев, ближе к диагонали матрицы. Чем ближе два вектора загрузки - тем сильнее их корреляция. Ортогональные векторы имеют корр. ноль, а тупые углы соответствуют отрицательной корр. 
# Принципиальные компоненты - это собственные вектора матрицы корреляций, которым соответсвтуют наибольшие собственные числа (это вектора, в базисе которых максимизируется дисперсия данных при их проецировании в этот базис).
# Это отдельная, очень большая тема, углубляться в которую здесь не имеет смысла, эта информация Вам просто для справки:)
# Для наглядности приведен график и матрица загрузки (ряды будут векторами загрузки):
pc <- princomp(dataset, cor = TRUE, score = TRUE)
pc$loadings
biplot(pc)
# Базисом пространства этого графика являются принципиальные компоненты, а векторы отражают влияние каждого столбца данных на дисперсию в базисе.
# Итак, на основе матрицы корреляций мы можем выбрать те регрессоры, которые имеют наибольшую корреляцию с зависимой переменной B.
# Обратите внимание что между выбранными регрессорами не должно быть сильного значения корреляции, иначе Вы столкнетесь с мультиколлинеарностью
# Выбираем регрессоры А1, А9, А6 и А11


# Для наглядности строим рассеянные графики, проверфм что нет аномалий в совместном распределении
plot(dataset$B, dataset$A9)
plot(dataset$B, dataset$A1)
plot(dataset$B, dataset$A6)
plot(dataset$B, dataset$A11)

# Этот шаг больше актуален для машинного обучения, нежели чем для регрессии. Мы хотим случайным образом разделить 
# наш датасет 70/30: чтобы найти коэффициенты на 70% и проверить их на оставшихся 30%. Я так сделал на семинаре,
# однако, впоследствие выяснилось что разделение существенно влияет на построение модели (либо из-за ненормализованности данных,
# либо из-за размера датасета и наличия экстремальных значений)
library(caret)
index <- createDataPartition(dataset$B, p = .70, list = FALSE) 
train <- dataset[index, ] 
test <- dataset[-index, ] 
# я оставил этот шаг в коде, но впоследствие я буду использовать полный датасет

# Строим модель с помощью команды lm(). Желаемая форма модели: B = alpha + Beta1*A1 + Beta2*A6 + Beta3*A9 + Beta4*A11
lin_mod <- lm(B~A1+A6+A9+A11, data = dataset)
# Модель построена и мы видим коэфициенты:
print(lin_mod)
# Но нам важно обратить внимание на суммарную статистику нашей модели:
summary(lin_mod)
# Смотрим, в первую очередь, на раздел Coefficients, в колонку Pr(>|t|). В общем и целом, в этой колонке
# отражены p-value для нулевых гипотез, которые заключаются в том, что Beta регрессора равна нулю. То есть, что регрессор
# не нужен для формирования модели. При данном пороге статистической значимости (к примеру, 5%) p-value сопоставляется со значением,
# и если p-value меньше, чем данный уровень, то мы говорим что нулевая гипотеза отвергается и регрессор является статистически значимым, и оставляем его в модели.
# Формально это будет звучать так: вероятность совершить ошибку первого рода при отвержении нулевой гипотезы составляет менее 5%, поэтому с 95% уверенностью регрессор является
# статистически значимым.
# Мы смотрим на результаты нашей модели и видим что регрессор А1 не попадает в интервал 5% (хотя и не выходит за рамки 10%)
# Вторым важным показателем будет являться R-squared и Adjusted R-squared. Показатель R-squared формируетсф из отношения
# 1-RSS/TSS, где RSS это фактически сумма остаточных ошибок в модели регрессии (эпсилон_i), а TSS это сумма разницы данных по столбцам с их средней.
# R-squared говорит нам о том, каков процент объясненной моделью дисперсии данных в общей дисперсии данных.
# То есть, насколько хорошо наша модель описывает тренировочные данные. Проблема R-squared в том, что его значение растет с количеством
# регрессоров, что не всегда соответствует улучшению предсказательной способности модели. Adjusted R-squared делает попытку
# компенсировать такое поведение.
# Насколько мы видим, наша модель имеет Adjusted R-squared 60,7%. Это не самый хороший показатель.
# Попробуем убрать регрессор с наименьшей статистической значимостью (А1):

lin_mod <- lm(B~A6+A9+A11, data = dataset)
print(lin_mod)
summary(lin_mod)
# Мы видим что все регрессоры здесь являются статистически значимыми при уровне 5%, однако Adjusted R-squared стал еще меньше - 59%.
# Можно также попробовать небольшую хитрость и подставить все регрессоры в модель, а потом посмотреть их значимость
# Стоит помнить, что такой способ не является предпочтительным в большинстве случаев, так как может привести к таким негативным
# последствиям как мультиколлинеарность в регрессорах (взаимная корреляция в регрессорах) и оверфиттингу( обманчиво хорошая описательная способность в тренировочных данных из за большого количества ненужных регрессоров), 
# что впоседствии негативно скажется на предсказательной способности модели
lin_mod <- lm(B~A1+A2+A3+A4+A5+A6+A7+A8+A9+A10+A11+A12+A13+A14+A15, data = dataset)
print(lin_mod)
summary(lin_mod)
# Мы видим что статистическую значимость имеют регрессоры А1, А2, А6, А8, А9
lin_mod <- lm(B~A1+A2+A6+A8+A9, data = dataset)
print(lin_mod)
summary(lin_mod)
# Мы видим что Adjusted R-squared находится в области 70%, что является достаточно хорошим показателем, однако 
# один из регрессоров стал статистически не значительным при уровне 5%. 
# Здесь мы можем иметь дело с недостаточно хорошо обработанными денными (нормализация и экстремальные значения)
# В начале нашего кода мы создали логарифмически трансформированный датасет. Его распределение ближе к нормальному, по крайней мере в независимой переменной.
# Давайте обратимся к нему (p.s. спасибо Александру за напоминание ;))

# В первую очередь, посмотрим вновь на матрицу корреляций:
c3 <- corrgram(dataset_log, order=TRUE, lower.panel = panel.pts, upper.panel = panel.pie)
# Обратите внимание как изменилось расположение колонок данных в матрице. Изменились и корреляции. Даже рассеянные графики поменяли вид (см. к примеру А12 и А13)
# Выберем наиболее коррелирующие с B регрессоры:
lin_mod <- lm(B~A1+A6+A9+A14+A11+A7+A14, data = dataset_log)
print(lin_mod)
summary(lin_mod)
# Мы сразу видим что Adj. R-sq. равен 65%, при том что 3 регрессора у нас не являются статистическими значимыми
# Те регрессоры что действительно помогают описать зависимую переменную имеют высокую статистическую значимость - 
# существенно меньше 1%. Это очень высокий уровень значимости. Изменяем модель, включая в нее только значимые регрессоры:
lin_mod <- lm(B~A1+A9+A14, data = dataset_log)
print(lin_mod)
summary(lin_mod)
# Итак, все регрессоры являются статистически значимыми даже в пределах 1% отсечки, и Adj. R-sq. остался на уровне 64%. Это неплохие показатели.
# P-value по F-statistic, которая определяет общую значимость модели, тоже менее 1%, что говорит о том, что модель является статистически значимой.
# Конечно, можно добится лучших показателей если серьезно поработать над трансформацией данных. Но для первого знакомства с регрессией хватит и этого. 
# Наконец, попробуем для наглядности "предсказать" зависимую переменную:
B_predict <- matrix(predict(lin_mod, dataset_log))
B_compare <- exp(cbind(dataset_log$B, B_predict))
View(B_compare)
# В первой колонке - оригинальные значения зависимой переменной, во второй - смоделированные.
# Значения, конечно, не идентичные, но вполне близкие.



